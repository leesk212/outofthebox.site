---
toc: True
tags: paper data-pipeline data-preprocessing
---
# 데이터 공학자의 업무란!?
* 자료원(데이터 공급원)에서 데이터를 질의하고[1. 추출과정] 데이터를 어떤 방식으로든 수정하고[2. 변환과정] 데이터를 사용자가 접근할 수 있는, 그리고 거기에 있는 데이터가 실무 품질임을 아는 어떤 장소에 넣는[3. 적재과정] 3가지를 진행하는 업무이다.
* 모든 지역(region)의 db를 연결해서 데이터를 추출하고, 데이터 웨어하우스에 적재한 후 그곳에서 원하는 데이터 값을 계산한다.

## 데이터 공학자에게 필요한 기술과 지식
1. db에서 데이터를 추출하는 방법(SQL, Python)
2. 데이터 파이프라인에서 데이터를 필요한 형태로 변환과정
3. 데이터를 적재할때 틀(스키마)를 만들기 위한 데이터 웨어하우스 과정
4. 마지막으로 이 모든 과정을 관리 및 infra를 구축할 수 있는 기술

## 데이터 공학자와 데이터 과학자
* 공생관계이지만 공학자는 데이터 과학자의 needs에 맞추어 데이터를 줘야하는 느낌(backend)
* 효율적인 데이터 전송을 위한 데이터형식, 모형, 구조를 고민

## 데이터 파이프라인
* 3v를 감당할 수 있는 도구들(volume,variety,velocity)를 사용
  * 매우 큰 크기의 data양
  * 다양한 곳에서 추출되는 다양한 format의 데이터
  * sns같은 0.1초 단위의 update되는 데이터의 실시간성 처리
### 도구
* db: redshift, bigquery, cassandra, **asticsearch**
* apache spark(python,RDD{immutable, distributed_collection)), apache Storm(Spout), apache Flink,Samza(unbounded stream)
### Definition
* 트랜잭션 데이터베이스와 프로그래밍 언어, 처리 엔진, 데이터 웨어하우스를 조합하면 하나의 파이프라인이 만들어진다.
* 파이프라인의 전체적인 과정을 매번 직접실행하면 효율성이 떨어지니, 일정한 주기로 파이프라인을 실행해 주는 스케줄러가 있어야한다.
  * crontab: X시간마다 python script를 실행하는 cron작업이 있으면 된다. 
  * 하지만 배압 문제가 발생할 수 있기에, 더 효율적인 파이프라인 또한 필요하다.
* 
